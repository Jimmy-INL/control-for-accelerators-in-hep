{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234885\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import dataprep.dataset as dp\n",
    "filename='MLParamData_1583906408.4261804_From_MLrn_2020-03-10+00_00_00_to_2020-03-11+00_00_00.h5_processed.csv.gz'\n",
    "nsteps=250000\n",
    "df = dp.load_reformated_cvs('../data/'+filename,nrows=nsteps)\n",
    "df = df.set_index(pd.to_datetime(df.time))\n",
    "df = df.dropna()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(164267, 1, 300)\n",
      "(164267, 2)\n",
      "(164267, 1, 750)\n",
      "(164267, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "## 1 second (cycle - 15Hz)\n",
    "look_back    = 10*15 \n",
    "look_forward = 1 \n",
    "    \n",
    "def create_dataset(dataset, look_back=1,look_forward=1):\n",
    "    X, Y = [], []\n",
    "    offset = look_back+look_forward\n",
    "    for i in range(len(dataset)-(offset+1)):\n",
    "        xx = dataset[i:(i+look_back), 0]\n",
    "        yy = dataset[(i + look_back):(i + offset), 0]\n",
    "        X.append(xx)\n",
    "        Y.append(yy)\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def get_dataset(variable='B:VIMIN'):\n",
    "\n",
    "    dataset = df[variable].values #numpy.ndarray\n",
    "    dataset = dataset.astype('float32')\n",
    "    dataset = np.reshape(dataset, (-1, 1))\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "    ## TODO: Fix\n",
    "    #print(len(dataset))\n",
    "    train_size = int(len(dataset) * 0.70)\n",
    "    #print(train_size)\n",
    "    test_size = len(dataset) - train_size\n",
    "    #print(test_size)\n",
    "\n",
    "    train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "    X_train, Y_train = create_dataset(train, look_back,look_forward)\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    Y_train = np.reshape(Y_train, (Y_train.shape[0],  Y_train.shape[1]))\n",
    "    #print(X_train.shape)\n",
    "    #print(Y_train.shape)\n",
    "    \n",
    "    X_test, Y_test = create_dataset(test, look_back,look_forward)\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "    Y_test = np.reshape(Y_test, (Y_test.shape[0],  Y_test.shape[1]))\n",
    "    #print(X_test.shape)\n",
    "    #print(Y_test.shape)\n",
    "    return scaler, X_train, Y_train, X_test, Y_test\n",
    "\n",
    "variables = ['B:VIMIN','B:IMINER','B:LINFRQ','I:IB','I:MDAT40']\n",
    "data_list = []\n",
    "for v in range(len(variables)):\n",
    "    data_list.append(get_dataset(variable=variables[v]))\n",
    "\n",
    "# Axis\n",
    "concate_axis=2\n",
    "## Injector model data\n",
    "InjX_train = np.concatenate((data_list[3][1],data_list[4][1]),axis=concate_axis)\n",
    "InjY_train = np.concatenate((data_list[3][2],data_list[4][2]),axis=1) \n",
    "InjX_test = np.concatenate((data_list[3][3],data_list[4][3]),axis=concate_axis)\n",
    "InjY_test = np.concatenate((data_list[3][4],data_list[4][4]),axis=1) \n",
    "print(InjX_train.shape)\n",
    "print(InjY_train.shape)\n",
    "\n",
    "## Booster model data\n",
    "BoX_train = np.concatenate((data_list[0][1],data_list[1][1],data_list[2][1],data_list[3][1],data_list[4][1]),axis=concate_axis) \n",
    "BoY_train = np.concatenate((data_list[0][2],data_list[1][2],data_list[2][2],data_list[3][2],data_list[4][2]),axis=1) \n",
    "BoX_test = np.concatenate((data_list[0][3],data_list[1][3],data_list[2][3],data_list[3][3],data_list[4][3]),axis=concate_axis) \n",
    "BoY_test = np.concatenate((data_list[0][4],data_list[1][4],data_list[2][4],data_list[3][4],data_list[4][4]),axis=1) \n",
    "print(BoX_train.shape)\n",
    "print(BoY_train.shape)\n",
    "from pickle import dump\n",
    "# save the scaler\n",
    "for v in range(len(variables)):\n",
    "    dump(data_list[v][0], open('scaler_var{}_nsteps{}k.pkl'.format(v,int(nsteps/1000)), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 256)            1031168   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1, 256)            525312    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 2,083,077\n",
      "Trainable params: 2,083,077\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/350\n",
      "1328/1328 - 17s - loss: 0.3831 - val_loss: 0.3426 - lr: 0.0100\n",
      "Epoch 2/350\n",
      "1328/1328 - 16s - loss: 0.3402 - val_loss: 0.3354 - lr: 0.0100\n",
      "Epoch 3/350\n",
      "1328/1328 - 16s - loss: 0.3401 - val_loss: 0.3457 - lr: 0.0100\n",
      "Epoch 4/350\n",
      "1328/1328 - 16s - loss: 0.3399 - val_loss: 0.3385 - lr: 0.0100\n",
      "Epoch 5/350\n",
      "1328/1328 - 16s - loss: 0.3399 - val_loss: 0.3400 - lr: 0.0100\n",
      "Epoch 6/350\n",
      "1328/1328 - 16s - loss: 0.3398 - val_loss: 0.3353 - lr: 0.0100\n",
      "Epoch 7/350\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.008499999810010194.\n",
      "1328/1328 - 16s - loss: 0.3398 - val_loss: 0.3445 - lr: 0.0100\n",
      "Epoch 8/350\n",
      "1328/1328 - 16s - loss: 0.2776 - val_loss: 0.2807 - lr: 0.0085\n",
      "Epoch 9/350\n",
      "1328/1328 - 16s - loss: 0.2788 - val_loss: 0.2825 - lr: 0.0085\n",
      "Epoch 10/350\n",
      "1328/1328 - 16s - loss: 0.2789 - val_loss: 0.2663 - lr: 0.0085\n",
      "Epoch 11/350\n",
      "1328/1328 - 16s - loss: 0.2788 - val_loss: 0.2887 - lr: 0.0085\n",
      "Epoch 12/350\n",
      "1328/1328 - 16s - loss: 0.2788 - val_loss: 0.2900 - lr: 0.0085\n",
      "Epoch 13/350\n",
      "1328/1328 - 16s - loss: 0.2788 - val_loss: 0.2734 - lr: 0.0085\n",
      "Epoch 14/350\n",
      "1328/1328 - 16s - loss: 0.2788 - val_loss: 0.2809 - lr: 0.0085\n",
      "Epoch 15/350\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.007224999601021409.\n",
      "1328/1328 - 16s - loss: 0.2788 - val_loss: 0.2827 - lr: 0.0085\n",
      "Epoch 16/350\n",
      "1328/1328 - 16s - loss: 0.2352 - val_loss: 0.2320 - lr: 0.0072\n",
      "Epoch 17/350\n",
      "1328/1328 - 16s - loss: 0.2366 - val_loss: 0.2335 - lr: 0.0072\n",
      "Epoch 18/350\n",
      "1328/1328 - 16s - loss: 0.2366 - val_loss: 0.2310 - lr: 0.0072\n",
      "Epoch 19/350\n",
      "1328/1328 - 16s - loss: 0.2366 - val_loss: 0.2365 - lr: 0.0072\n",
      "Epoch 20/350\n",
      "1328/1328 - 16s - loss: 0.2366 - val_loss: 0.2329 - lr: 0.0072\n",
      "Epoch 21/350\n",
      "1328/1328 - 16s - loss: 0.2366 - val_loss: 0.2366 - lr: 0.0072\n",
      "Epoch 22/350\n",
      "1328/1328 - 16s - loss: 0.2366 - val_loss: 0.2355 - lr: 0.0072\n",
      "Epoch 23/350\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00614124946296215.\n",
      "1328/1328 - 16s - loss: 0.2366 - val_loss: 0.2391 - lr: 0.0072\n",
      "Epoch 24/350\n",
      "1328/1328 - 16s - loss: 0.2050 - val_loss: 0.2027 - lr: 0.0061\n",
      "Epoch 25/350\n",
      "1328/1328 - 16s - loss: 0.2062 - val_loss: 0.2092 - lr: 0.0061\n",
      "Epoch 26/350\n",
      "1328/1328 - 16s - loss: 0.2062 - val_loss: 0.1999 - lr: 0.0061\n",
      "Epoch 27/350\n",
      "1328/1328 - 16s - loss: 0.2063 - val_loss: 0.2066 - lr: 0.0061\n",
      "Epoch 28/350\n",
      "1328/1328 - 16s - loss: 0.2063 - val_loss: 0.2071 - lr: 0.0061\n",
      "Epoch 29/350\n",
      "1328/1328 - 16s - loss: 0.2063 - val_loss: 0.2036 - lr: 0.0061\n",
      "Epoch 30/350\n",
      "1328/1328 - 16s - loss: 0.2063 - val_loss: 0.2050 - lr: 0.0061\n",
      "Epoch 31/350\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.005220062122680246.\n",
      "1328/1328 - 16s - loss: 0.2062 - val_loss: 0.2133 - lr: 0.0061\n",
      "Epoch 32/350\n",
      "1328/1328 - 16s - loss: 0.1780 - val_loss: 0.1748 - lr: 0.0052\n",
      "Epoch 33/350\n",
      "1328/1328 - 16s - loss: 0.1784 - val_loss: 0.1789 - lr: 0.0052\n",
      "Epoch 34/350\n",
      "1328/1328 - 16s - loss: 0.1785 - val_loss: 0.1751 - lr: 0.0052\n",
      "Epoch 35/350\n",
      "1328/1328 - 16s - loss: 0.1785 - val_loss: 0.1770 - lr: 0.0052\n",
      "Epoch 36/350\n",
      "1328/1328 - 16s - loss: 0.1784 - val_loss: 0.1738 - lr: 0.0052\n",
      "Epoch 37/350\n",
      "1328/1328 - 16s - loss: 0.1785 - val_loss: 0.1769 - lr: 0.0052\n",
      "Epoch 38/350\n",
      "1328/1328 - 16s - loss: 0.1785 - val_loss: 0.1760 - lr: 0.0052\n",
      "Epoch 39/350\n",
      "1328/1328 - 16s - loss: 0.1784 - val_loss: 0.1735 - lr: 0.0052\n",
      "Epoch 40/350\n",
      "1328/1328 - 16s - loss: 0.1784 - val_loss: 0.1801 - lr: 0.0052\n",
      "Epoch 41/350\n",
      "1328/1328 - 16s - loss: 0.1785 - val_loss: 0.1747 - lr: 0.0052\n",
      "Epoch 42/350\n",
      "1328/1328 - 16s - loss: 0.1784 - val_loss: 0.1768 - lr: 0.0052\n",
      "Epoch 43/350\n",
      "1328/1328 - 16s - loss: 0.1784 - val_loss: 0.1788 - lr: 0.0052\n",
      "Epoch 44/350\n",
      "1328/1328 - 16s - loss: 0.1784 - val_loss: 0.1718 - lr: 0.0052\n",
      "Epoch 45/350\n",
      "1328/1328 - 16s - loss: 0.1785 - val_loss: 0.1744 - lr: 0.0052\n",
      "Epoch 46/350\n",
      "1328/1328 - 16s - loss: 0.1784 - val_loss: 0.1786 - lr: 0.0052\n",
      "Epoch 47/350\n",
      "1328/1328 - 16s - loss: 0.1785 - val_loss: 0.1766 - lr: 0.0052\n",
      "Epoch 48/350\n",
      "1328/1328 - 16s - loss: 0.1784 - val_loss: 0.1752 - lr: 0.0052\n",
      "Epoch 49/350\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.004437052784487605.\n",
      "1328/1328 - 16s - loss: 0.1785 - val_loss: 0.1795 - lr: 0.0052\n",
      "Epoch 50/350\n",
      "1328/1328 - 16s - loss: 0.1539 - val_loss: 0.1503 - lr: 0.0044\n",
      "Epoch 51/350\n",
      "1328/1328 - 16s - loss: 0.1542 - val_loss: 0.1529 - lr: 0.0044\n",
      "Epoch 52/350\n",
      "1328/1328 - 16s - loss: 0.1542 - val_loss: 0.1539 - lr: 0.0044\n",
      "Epoch 53/350\n",
      "1328/1328 - 16s - loss: 0.1542 - val_loss: 0.1525 - lr: 0.0044\n",
      "Epoch 54/350\n",
      "1328/1328 - 16s - loss: 0.1542 - val_loss: 0.1508 - lr: 0.0044\n",
      "Epoch 55/350\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.003771494748070836.\n",
      "1328/1328 - 16s - loss: 0.1542 - val_loss: 0.1538 - lr: 0.0044\n",
      "Epoch 56/350\n",
      "1328/1328 - 16s - loss: 0.1323 - val_loss: 0.1303 - lr: 0.0038\n",
      "Epoch 57/350\n",
      "1328/1328 - 16s - loss: 0.1324 - val_loss: 0.1301 - lr: 0.0038\n",
      "Epoch 58/350\n",
      "1328/1328 - 16s - loss: 0.1324 - val_loss: 0.1336 - lr: 0.0038\n",
      "Epoch 59/350\n",
      "1328/1328 - 16s - loss: 0.1324 - val_loss: 0.1284 - lr: 0.0038\n",
      "Epoch 60/350\n",
      "1328/1328 - 16s - loss: 0.1324 - val_loss: 0.1300 - lr: 0.0038\n",
      "Epoch 61/350\n",
      "1328/1328 - 16s - loss: 0.1324 - val_loss: 0.1331 - lr: 0.0038\n",
      "Epoch 62/350\n",
      "1328/1328 - 16s - loss: 0.1324 - val_loss: 0.1291 - lr: 0.0038\n",
      "Epoch 63/350\n",
      "1328/1328 - 16s - loss: 0.1323 - val_loss: 0.1317 - lr: 0.0038\n",
      "Epoch 64/350\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 0.0032057706150226293.\n",
      "1328/1328 - 16s - loss: 0.1324 - val_loss: 0.1313 - lr: 0.0038\n",
      "Epoch 65/350\n",
      "1328/1328 - 16s - loss: 0.1137 - val_loss: 0.1108 - lr: 0.0032\n",
      "Epoch 66/350\n",
      "1328/1328 - 16s - loss: 0.1137 - val_loss: 0.1127 - lr: 0.0032\n",
      "Epoch 67/350\n",
      "1328/1328 - 16s - loss: 0.1137 - val_loss: 0.1132 - lr: 0.0032\n",
      "Epoch 68/350\n",
      "1328/1328 - 16s - loss: 0.1137 - val_loss: 0.1124 - lr: 0.0032\n",
      "Epoch 69/350\n",
      "1328/1328 - 16s - loss: 0.1137 - val_loss: 0.1111 - lr: 0.0032\n",
      "Epoch 70/350\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 0.0027249050326645374.\n",
      "1328/1328 - 16s - loss: 0.1137 - val_loss: 0.1135 - lr: 0.0032\n",
      "Epoch 71/350\n",
      "1328/1328 - 16s - loss: 0.0978 - val_loss: 0.0963 - lr: 0.0027\n",
      "Epoch 72/350\n",
      "1328/1328 - 16s - loss: 0.0978 - val_loss: 0.0962 - lr: 0.0027\n",
      "Epoch 73/350\n",
      "1328/1328 - 16s - loss: 0.0978 - val_loss: 0.0987 - lr: 0.0027\n",
      "Epoch 74/350\n",
      "1328/1328 - 16s - loss: 0.0978 - val_loss: 0.0948 - lr: 0.0027\n",
      "Epoch 75/350\n",
      "1328/1328 - 16s - loss: 0.0978 - val_loss: 0.0961 - lr: 0.0027\n",
      "Epoch 76/350\n",
      "1328/1328 - 16s - loss: 0.0978 - val_loss: 0.0982 - lr: 0.0027\n",
      "Epoch 77/350\n",
      "1328/1328 - 16s - loss: 0.0978 - val_loss: 0.0955 - lr: 0.0027\n",
      "Epoch 78/350\n",
      "1328/1328 - 16s - loss: 0.0978 - val_loss: 0.0973 - lr: 0.0027\n",
      "Epoch 79/350\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 0.0023161692777648566.\n",
      "1328/1328 - 16s - loss: 0.0978 - val_loss: 0.0968 - lr: 0.0027\n",
      "Epoch 80/350\n",
      "1328/1328 - 16s - loss: 0.0843 - val_loss: 0.0821 - lr: 0.0023\n",
      "Epoch 81/350\n",
      "1328/1328 - 16s - loss: 0.0843 - val_loss: 0.0836 - lr: 0.0023\n",
      "Epoch 82/350\n",
      "1328/1328 - 16s - loss: 0.0843 - val_loss: 0.0840 - lr: 0.0023\n",
      "Epoch 83/350\n",
      "1328/1328 - 16s - loss: 0.0843 - val_loss: 0.0833 - lr: 0.0023\n",
      "Epoch 84/350\n",
      "1328/1328 - 16s - loss: 0.0843 - val_loss: 0.0824 - lr: 0.0023\n",
      "Epoch 85/350\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 0.001968743826728314.\n",
      "1328/1328 - 16s - loss: 0.0843 - val_loss: 0.0839 - lr: 0.0023\n",
      "Epoch 86/350\n",
      "1328/1328 - 16s - loss: 0.0728 - val_loss: 0.0716 - lr: 0.0020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/350\n",
      "1328/1328 - 16s - loss: 0.0728 - val_loss: 0.0716 - lr: 0.0020\n",
      "Epoch 88/350\n",
      "1328/1328 - 16s - loss: 0.0728 - val_loss: 0.0733 - lr: 0.0020\n",
      "Epoch 89/350\n",
      "1328/1328 - 16s - loss: 0.0728 - val_loss: 0.0705 - lr: 0.0020\n",
      "Epoch 90/350\n",
      "1328/1328 - 16s - loss: 0.0728 - val_loss: 0.0715 - lr: 0.0020\n",
      "Epoch 91/350\n",
      "1328/1328 - 16s - loss: 0.0728 - val_loss: 0.0731 - lr: 0.0020\n",
      "Epoch 92/350\n",
      "1328/1328 - 16s - loss: 0.0728 - val_loss: 0.0711 - lr: 0.0020\n",
      "Epoch 93/350\n",
      "1328/1328 - 16s - loss: 0.0728 - val_loss: 0.0724 - lr: 0.0020\n",
      "Epoch 94/350\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 0.0016734321834519506.\n",
      "1328/1328 - 16s - loss: 0.0728 - val_loss: 0.0720 - lr: 0.0020\n",
      "Epoch 95/350\n",
      "1328/1328 - 16s - loss: 0.0631 - val_loss: 0.0615 - lr: 0.0017\n",
      "Epoch 96/350\n",
      "1328/1328 - 16s - loss: 0.0631 - val_loss: 0.0624 - lr: 0.0017\n",
      "Epoch 97/350\n",
      "1328/1328 - 16s - loss: 0.0631 - val_loss: 0.0627 - lr: 0.0017\n",
      "Epoch 98/350\n",
      "1328/1328 - 16s - loss: 0.0631 - val_loss: 0.0622 - lr: 0.0017\n",
      "Epoch 99/350\n",
      "1328/1328 - 16s - loss: 0.0631 - val_loss: 0.0616 - lr: 0.0017\n",
      "Epoch 100/350\n",
      "\n",
      "Epoch 00100: ReduceLROnPlateau reducing learning rate to 0.0014224173559341578.\n",
      "1328/1328 - 16s - loss: 0.0631 - val_loss: 0.0628 - lr: 0.0017\n",
      "Epoch 101/350\n",
      "1328/1328 - 16s - loss: 0.0548 - val_loss: 0.0540 - lr: 0.0014\n",
      "Epoch 102/350\n",
      "1328/1328 - 16s - loss: 0.0548 - val_loss: 0.0538 - lr: 0.0014\n",
      "Epoch 103/350\n",
      "1328/1328 - 16s - loss: 0.0548 - val_loss: 0.0551 - lr: 0.0014\n",
      "Epoch 104/350\n",
      "1328/1328 - 16s - loss: 0.0548 - val_loss: 0.0531 - lr: 0.0014\n",
      "Epoch 105/350\n",
      "1328/1328 - 16s - loss: 0.0548 - val_loss: 0.0537 - lr: 0.0014\n",
      "Epoch 106/350\n",
      "1328/1328 - 16s - loss: 0.0548 - val_loss: 0.0549 - lr: 0.0014\n",
      "Epoch 107/350\n",
      "1328/1328 - 16s - loss: 0.0548 - val_loss: 0.0534 - lr: 0.0014\n",
      "Epoch 108/350\n",
      "1328/1328 - 16s - loss: 0.0548 - val_loss: 0.0544 - lr: 0.0014\n",
      "Epoch 109/350\n",
      "\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 0.001209054747596383.\n",
      "1328/1328 - 16s - loss: 0.0548 - val_loss: 0.0542 - lr: 0.0014\n",
      "Epoch 110/350\n",
      "1328/1328 - 16s - loss: 0.0477 - val_loss: 0.0467 - lr: 0.0012\n",
      "Epoch 111/350\n",
      "1328/1328 - 16s - loss: 0.0477 - val_loss: 0.0472 - lr: 0.0012\n",
      "Epoch 112/350\n",
      "1328/1328 - 16s - loss: 0.0477 - val_loss: 0.0474 - lr: 0.0012\n",
      "Epoch 113/350\n",
      "1328/1328 - 16s - loss: 0.0477 - val_loss: 0.0471 - lr: 0.0012\n",
      "Epoch 114/350\n",
      "1328/1328 - 16s - loss: 0.0477 - val_loss: 0.0466 - lr: 0.0012\n",
      "Epoch 115/350\n",
      "\n",
      "Epoch 00115: ReduceLROnPlateau reducing learning rate to 0.0010276965156663209.\n",
      "1328/1328 - 16s - loss: 0.0477 - val_loss: 0.0474 - lr: 0.0012\n",
      "Epoch 116/350\n",
      "1328/1328 - 16s - loss: 0.0418 - val_loss: 0.0413 - lr: 0.0010\n",
      "Epoch 117/350\n",
      "1328/1328 - 16s - loss: 0.0418 - val_loss: 0.0410 - lr: 0.0010\n",
      "Epoch 118/350\n",
      "1328/1328 - 16s - loss: 0.0418 - val_loss: 0.0419 - lr: 0.0010\n",
      "Epoch 119/350\n",
      "1328/1328 - 16s - loss: 0.0418 - val_loss: 0.0405 - lr: 0.0010\n",
      "Epoch 120/350\n",
      "1328/1328 - 16s - loss: 0.0418 - val_loss: 0.0410 - lr: 0.0010\n",
      "Epoch 121/350\n",
      "1328/1328 - 16s - loss: 0.0418 - val_loss: 0.0418 - lr: 0.0010\n",
      "Epoch 122/350\n",
      "1328/1328 - 16s - loss: 0.0418 - val_loss: 0.0407 - lr: 0.0010\n",
      "Epoch 123/350\n",
      "1328/1328 - 16s - loss: 0.0418 - val_loss: 0.0415 - lr: 0.0010\n",
      "Epoch 124/350\n",
      "\n",
      "Epoch 00124: ReduceLROnPlateau reducing learning rate to 0.0008735420531593263.\n",
      "1328/1328 - 16s - loss: 0.0418 - val_loss: 0.0413 - lr: 0.0010\n",
      "Epoch 125/350\n",
      "1328/1328 - 16s - loss: 0.0367 - val_loss: 0.0359 - lr: 8.7354e-04\n",
      "Epoch 126/350\n",
      "1328/1328 - 16s - loss: 0.0367 - val_loss: 0.0362 - lr: 8.7354e-04\n",
      "Epoch 127/350\n",
      "1328/1328 - 16s - loss: 0.0367 - val_loss: 0.0364 - lr: 8.7354e-04\n",
      "Epoch 128/350\n",
      "1328/1328 - 16s - loss: 0.0367 - val_loss: 0.0361 - lr: 8.7354e-04\n",
      "Epoch 129/350\n",
      "1328/1328 - 16s - loss: 0.0367 - val_loss: 0.0358 - lr: 8.7354e-04\n",
      "Epoch 130/350\n",
      "1328/1328 - 16s - loss: 0.0367 - val_loss: 0.0363 - lr: 8.7354e-04\n",
      "Epoch 131/350\n",
      "1328/1328 - 16s - loss: 0.0367 - val_loss: 0.0360 - lr: 8.7354e-04\n",
      "Epoch 132/350\n",
      "1328/1328 - 16s - loss: 0.0367 - val_loss: 0.0360 - lr: 8.7354e-04\n",
      "Epoch 133/350\n",
      "1328/1328 - 16s - loss: 0.0367 - val_loss: 0.0368 - lr: 8.7354e-04\n",
      "Epoch 134/350\n",
      "1328/1328 - 16s - loss: 0.0367 - val_loss: 0.0355 - lr: 8.7354e-04\n",
      "Epoch 135/350\n",
      "1328/1328 - 16s - loss: 0.0367 - val_loss: 0.0359 - lr: 8.7354e-04\n",
      "Epoch 136/350\n",
      "1328/1328 - 16s - loss: 0.0367 - val_loss: 0.0367 - lr: 8.7354e-04\n",
      "Epoch 137/350\n",
      "1328/1328 - 16s - loss: 0.0367 - val_loss: 0.0357 - lr: 8.7354e-04\n",
      "Epoch 138/350\n",
      "1328/1328 - 16s - loss: 0.0367 - val_loss: 0.0363 - lr: 8.7354e-04\n",
      "Epoch 139/350\n",
      "\n",
      "Epoch 00139: ReduceLROnPlateau reducing learning rate to 0.000742510735290125.\n",
      "1328/1328 - 16s - loss: 0.0367 - val_loss: 0.0362 - lr: 8.7354e-04\n",
      "Epoch 140/350\n",
      "1328/1328 - 16s - loss: 0.0323 - val_loss: 0.0317 - lr: 7.4251e-04\n",
      "Epoch 141/350\n",
      "1328/1328 - 16s - loss: 0.0323 - val_loss: 0.0320 - lr: 7.4251e-04\n",
      "Epoch 142/350\n",
      "1328/1328 - 16s - loss: 0.0323 - val_loss: 0.0321 - lr: 7.4251e-04\n",
      "Epoch 143/350\n",
      "1328/1328 - 16s - loss: 0.0323 - val_loss: 0.0318 - lr: 7.4251e-04\n",
      "Epoch 144/350\n",
      "1328/1328 - 16s - loss: 0.0323 - val_loss: 0.0315 - lr: 7.4251e-04\n",
      "Epoch 145/350\n",
      "1328/1328 - 16s - loss: 0.0323 - val_loss: 0.0320 - lr: 7.4251e-04\n",
      "Epoch 146/350\n",
      "1328/1328 - 16s - loss: 0.0323 - val_loss: 0.0317 - lr: 7.4251e-04\n",
      "Epoch 147/350\n",
      "1328/1328 - 16s - loss: 0.0323 - val_loss: 0.0317 - lr: 7.4251e-04\n",
      "Epoch 148/350\n",
      "1328/1328 - 16s - loss: 0.0323 - val_loss: 0.0324 - lr: 7.4251e-04\n",
      "Epoch 149/350\n",
      "1328/1328 - 16s - loss: 0.0323 - val_loss: 0.0313 - lr: 7.4251e-04\n",
      "Epoch 150/350\n",
      "1328/1328 - 16s - loss: 0.0323 - val_loss: 0.0317 - lr: 7.4251e-04\n",
      "Epoch 151/350\n",
      "1328/1328 - 16s - loss: 0.0323 - val_loss: 0.0323 - lr: 7.4251e-04\n",
      "Epoch 152/350\n",
      "1328/1328 - 16s - loss: 0.0323 - val_loss: 0.0315 - lr: 7.4251e-04\n",
      "Epoch 153/350\n",
      "1328/1328 - 16s - loss: 0.0323 - val_loss: 0.0320 - lr: 7.4251e-04\n",
      "Epoch 154/350\n",
      "\n",
      "Epoch 00154: ReduceLROnPlateau reducing learning rate to 0.0006311341101536527.\n",
      "1328/1328 - 16s - loss: 0.0323 - val_loss: 0.0319 - lr: 7.4251e-04\n",
      "Epoch 155/350\n",
      "1328/1328 - 16s - loss: 0.0287 - val_loss: 0.0282 - lr: 6.3113e-04\n",
      "Epoch 156/350\n",
      "1328/1328 - 16s - loss: 0.0287 - val_loss: 0.0283 - lr: 6.3113e-04\n",
      "Epoch 157/350\n",
      "1328/1328 - 16s - loss: 0.0287 - val_loss: 0.0284 - lr: 6.3113e-04\n",
      "Epoch 158/350\n",
      "1328/1328 - 16s - loss: 0.0287 - val_loss: 0.0282 - lr: 6.3113e-04\n",
      "Epoch 159/350\n",
      "1328/1328 - 16s - loss: 0.0287 - val_loss: 0.0279 - lr: 6.3113e-04\n",
      "Epoch 160/350\n",
      "1328/1328 - 16s - loss: 0.0287 - val_loss: 0.0283 - lr: 6.3113e-04\n",
      "Epoch 161/350\n",
      "1328/1328 - 16s - loss: 0.0287 - val_loss: 0.0281 - lr: 6.3113e-04\n",
      "Epoch 162/350\n",
      "1328/1328 - 16s - loss: 0.0287 - val_loss: 0.0281 - lr: 6.3113e-04\n",
      "Epoch 163/350\n",
      "1328/1328 - 15s - loss: 0.0287 - val_loss: 0.0286 - lr: 6.3113e-04\n",
      "Epoch 164/350\n",
      "1328/1328 - 15s - loss: 0.0287 - val_loss: 0.0278 - lr: 6.3113e-04\n",
      "Epoch 165/350\n",
      "1328/1328 - 15s - loss: 0.0287 - val_loss: 0.0280 - lr: 6.3113e-04\n",
      "Epoch 166/350\n",
      "1328/1328 - 15s - loss: 0.0287 - val_loss: 0.0286 - lr: 6.3113e-04\n",
      "Epoch 167/350\n",
      "1328/1328 - 15s - loss: 0.0287 - val_loss: 0.0279 - lr: 6.3113e-04\n",
      "Epoch 168/350\n",
      "1328/1328 - 16s - loss: 0.0287 - val_loss: 0.0283 - lr: 6.3113e-04\n",
      "Epoch 169/350\n",
      "\n",
      "Epoch 00169: ReduceLROnPlateau reducing learning rate to 0.0005364639713661745.\n",
      "1328/1328 - 16s - loss: 0.0287 - val_loss: 0.0282 - lr: 6.3113e-04\n",
      "Epoch 170/350\n",
      "1328/1328 - 16s - loss: 0.0255 - val_loss: 0.0252 - lr: 5.3646e-04\n",
      "Epoch 171/350\n",
      "1328/1328 - 16s - loss: 0.0255 - val_loss: 0.0252 - lr: 5.3646e-04\n",
      "Epoch 172/350\n",
      "1328/1328 - 16s - loss: 0.0255 - val_loss: 0.0253 - lr: 5.3646e-04\n",
      "Epoch 173/350\n",
      "1328/1328 - 16s - loss: 0.0255 - val_loss: 0.0251 - lr: 5.3646e-04\n",
      "Epoch 174/350\n",
      "1328/1328 - 16s - loss: 0.0255 - val_loss: 0.0249 - lr: 5.3646e-04\n",
      "Epoch 175/350\n",
      "1328/1328 - 16s - loss: 0.0255 - val_loss: 0.0252 - lr: 5.3646e-04\n",
      "Epoch 176/350\n",
      "1328/1328 - 16s - loss: 0.0255 - val_loss: 0.0250 - lr: 5.3646e-04\n",
      "Epoch 177/350\n",
      "1328/1328 - 15s - loss: 0.0255 - val_loss: 0.0250 - lr: 5.3646e-04\n",
      "Epoch 178/350\n",
      "1328/1328 - 16s - loss: 0.0255 - val_loss: 0.0255 - lr: 5.3646e-04\n",
      "Epoch 179/350\n",
      "1328/1328 - 16s - loss: 0.0255 - val_loss: 0.0247 - lr: 5.3646e-04\n",
      "Epoch 180/350\n",
      "1328/1328 - 16s - loss: 0.0255 - val_loss: 0.0250 - lr: 5.3646e-04\n",
      "Epoch 181/350\n",
      "1328/1328 - 16s - loss: 0.0255 - val_loss: 0.0254 - lr: 5.3646e-04\n",
      "Epoch 182/350\n",
      "1328/1328 - 16s - loss: 0.0255 - val_loss: 0.0249 - lr: 5.3646e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/350\n",
      "1328/1328 - 16s - loss: 0.0255 - val_loss: 0.0252 - lr: 5.3646e-04\n",
      "Epoch 184/350\n",
      "\n",
      "Epoch 00184: ReduceLROnPlateau reducing learning rate to 0.0004559943830827251.\n",
      "1328/1328 - 16s - loss: 0.0255 - val_loss: 0.0251 - lr: 5.3646e-04\n",
      "Epoch 185/350\n",
      "1328/1328 - 16s - loss: 0.0229 - val_loss: 0.0226 - lr: 4.5599e-04\n",
      "Epoch 186/350\n",
      "1328/1328 - 16s - loss: 0.0229 - val_loss: 0.0226 - lr: 4.5599e-04\n",
      "Epoch 187/350\n",
      "1328/1328 - 16s - loss: 0.0229 - val_loss: 0.0227 - lr: 4.5599e-04\n",
      "Epoch 188/350\n",
      "1328/1328 - 16s - loss: 0.0229 - val_loss: 0.0225 - lr: 4.5599e-04\n",
      "Epoch 189/350\n",
      "1328/1328 - 16s - loss: 0.0229 - val_loss: 0.0223 - lr: 4.5599e-04\n",
      "Epoch 190/350\n",
      "1328/1328 - 16s - loss: 0.0229 - val_loss: 0.0226 - lr: 4.5599e-04\n",
      "Epoch 191/350\n",
      "1328/1328 - 16s - loss: 0.0229 - val_loss: 0.0224 - lr: 4.5599e-04\n",
      "Epoch 192/350\n",
      "1328/1328 - 16s - loss: 0.0229 - val_loss: 0.0224 - lr: 4.5599e-04\n",
      "Epoch 193/350\n",
      "1328/1328 - 16s - loss: 0.0229 - val_loss: 0.0228 - lr: 4.5599e-04\n",
      "Epoch 194/350\n",
      "1328/1328 - 16s - loss: 0.0229 - val_loss: 0.0222 - lr: 4.5599e-04\n",
      "Epoch 195/350\n",
      "1328/1328 - 16s - loss: 0.0229 - val_loss: 0.0224 - lr: 4.5599e-04\n",
      "Epoch 196/350\n",
      "1328/1328 - 16s - loss: 0.0229 - val_loss: 0.0227 - lr: 4.5599e-04\n",
      "Epoch 197/350\n",
      "1328/1328 - 16s - loss: 0.0229 - val_loss: 0.0223 - lr: 4.5599e-04\n",
      "Epoch 198/350\n",
      "1328/1328 - 16s - loss: 0.0229 - val_loss: 0.0226 - lr: 4.5599e-04\n",
      "Epoch 199/350\n",
      "\n",
      "Epoch 00199: ReduceLROnPlateau reducing learning rate to 0.0003875952330417931.\n",
      "1328/1328 - 16s - loss: 0.0229 - val_loss: 0.0225 - lr: 4.5599e-04\n",
      "Epoch 200/350\n",
      "1328/1328 - 16s - loss: 0.0206 - val_loss: 0.0204 - lr: 3.8760e-04\n",
      "Epoch 201/350\n",
      "1328/1328 - 16s - loss: 0.0206 - val_loss: 0.0204 - lr: 3.8760e-04\n",
      "Epoch 202/350\n",
      "1328/1328 - 16s - loss: 0.0206 - val_loss: 0.0204 - lr: 3.8760e-04\n",
      "Epoch 203/350\n",
      "1328/1328 - 16s - loss: 0.0206 - val_loss: 0.0203 - lr: 3.8760e-04\n",
      "Epoch 204/350\n",
      "1328/1328 - 16s - loss: 0.0206 - val_loss: 0.0201 - lr: 3.8760e-04\n",
      "Epoch 205/350\n",
      "1328/1328 - 16s - loss: 0.0206 - val_loss: 0.0203 - lr: 3.8760e-04\n",
      "Epoch 206/350\n",
      "1328/1328 - 16s - loss: 0.0206 - val_loss: 0.0202 - lr: 3.8760e-04\n",
      "Epoch 207/350\n",
      "1328/1328 - 16s - loss: 0.0206 - val_loss: 0.0202 - lr: 3.8760e-04\n",
      "Epoch 208/350\n",
      "1328/1328 - 16s - loss: 0.0206 - val_loss: 0.0205 - lr: 3.8760e-04\n",
      "Epoch 209/350\n",
      "\n",
      "Epoch 00209: ReduceLROnPlateau reducing learning rate to 0.0003294559530331753.\n",
      "1328/1328 - 16s - loss: 0.0206 - val_loss: 0.0200 - lr: 3.8760e-04\n",
      "Epoch 210/350\n",
      "1328/1328 - 16s - loss: 0.0187 - val_loss: 0.0189 - lr: 3.2946e-04\n",
      "Epoch 211/350\n",
      "1328/1328 - 16s - loss: 0.0187 - val_loss: 0.0187 - lr: 3.2946e-04\n",
      "Epoch 212/350\n",
      "1328/1328 - 16s - loss: 0.0187 - val_loss: 0.0182 - lr: 3.2946e-04\n",
      "Epoch 213/350\n",
      "1328/1328 - 16s - loss: 0.0187 - val_loss: 0.0184 - lr: 3.2946e-04\n",
      "Epoch 214/350\n",
      "1328/1328 - 16s - loss: 0.0187 - val_loss: 0.0184 - lr: 3.2946e-04\n",
      "Epoch 215/350\n",
      "1328/1328 - 16s - loss: 0.0187 - val_loss: 0.0183 - lr: 3.2946e-04\n",
      "Epoch 216/350\n",
      "1328/1328 - 16s - loss: 0.0187 - val_loss: 0.0184 - lr: 3.2946e-04\n",
      "Epoch 217/350\n",
      "\n",
      "Epoch 00217: ReduceLROnPlateau reducing learning rate to 0.00028003755141980946.\n",
      "1328/1328 - 16s - loss: 0.0187 - val_loss: 0.0187 - lr: 3.2946e-04\n",
      "Epoch 218/350\n",
      "1328/1328 - 16s - loss: 0.0171 - val_loss: 0.0171 - lr: 2.8004e-04\n",
      "Epoch 219/350\n",
      "1328/1328 - 16s - loss: 0.0171 - val_loss: 0.0167 - lr: 2.8004e-04\n",
      "Epoch 220/350\n",
      "1328/1328 - 16s - loss: 0.0171 - val_loss: 0.0169 - lr: 2.8004e-04\n",
      "Epoch 221/350\n",
      "1328/1328 - 16s - loss: 0.0171 - val_loss: 0.0167 - lr: 2.8004e-04\n",
      "Epoch 222/350\n",
      "1328/1328 - 16s - loss: 0.0171 - val_loss: 0.0167 - lr: 2.8004e-04\n",
      "Epoch 223/350\n",
      "1328/1328 - 16s - loss: 0.0171 - val_loss: 0.0170 - lr: 2.8004e-04\n",
      "Epoch 224/350\n",
      "1328/1328 - 16s - loss: 0.0171 - val_loss: 0.0165 - lr: 2.8004e-04\n",
      "Epoch 225/350\n",
      "1328/1328 - 16s - loss: 0.0171 - val_loss: 0.0167 - lr: 2.8004e-04\n",
      "Epoch 226/350\n",
      "1328/1328 - 16s - loss: 0.0171 - val_loss: 0.0169 - lr: 2.8004e-04\n",
      "Epoch 227/350\n",
      "1328/1328 - 16s - loss: 0.0171 - val_loss: 0.0166 - lr: 2.8004e-04\n",
      "Epoch 228/350\n",
      "1328/1328 - 16s - loss: 0.0171 - val_loss: 0.0168 - lr: 2.8004e-04\n",
      "Epoch 229/350\n",
      "\n",
      "Epoch 00229: ReduceLROnPlateau reducing learning rate to 0.00023803191870683805.\n",
      "1328/1328 - 16s - loss: 0.0171 - val_loss: 0.0167 - lr: 2.8004e-04\n",
      "Epoch 230/350\n",
      "1328/1328 - 16s - loss: 0.0157 - val_loss: 0.0156 - lr: 2.3803e-04\n",
      "Epoch 231/350\n",
      "1328/1328 - 16s - loss: 0.0157 - val_loss: 0.0155 - lr: 2.3803e-04\n",
      "Epoch 232/350\n",
      "1328/1328 - 16s - loss: 0.0157 - val_loss: 0.0156 - lr: 2.3803e-04\n",
      "Epoch 233/350\n",
      "1328/1328 - 16s - loss: 0.0157 - val_loss: 0.0154 - lr: 2.3803e-04\n",
      "Epoch 234/350\n",
      "1328/1328 - 16s - loss: 0.0157 - val_loss: 0.0153 - lr: 2.3803e-04\n",
      "Epoch 235/350\n",
      "1328/1328 - 16s - loss: 0.0157 - val_loss: 0.0154 - lr: 2.3803e-04\n",
      "Epoch 236/350\n",
      "1328/1328 - 16s - loss: 0.0157 - val_loss: 0.0153 - lr: 2.3803e-04\n",
      "Epoch 237/350\n",
      "1328/1328 - 16s - loss: 0.0157 - val_loss: 0.0153 - lr: 2.3803e-04\n",
      "Epoch 238/350\n",
      "1328/1328 - 16s - loss: 0.0157 - val_loss: 0.0155 - lr: 2.3803e-04\n",
      "Epoch 239/350\n",
      "1328/1328 - 16s - loss: 0.0157 - val_loss: 0.0152 - lr: 2.3803e-04\n",
      "Epoch 240/350\n",
      "1328/1328 - 16s - loss: 0.0157 - val_loss: 0.0153 - lr: 2.3803e-04\n",
      "Epoch 241/350\n",
      "1328/1328 - 16s - loss: 0.0157 - val_loss: 0.0155 - lr: 2.3803e-04\n",
      "Epoch 242/350\n",
      "1328/1328 - 16s - loss: 0.0157 - val_loss: 0.0153 - lr: 2.3803e-04\n",
      "Epoch 243/350\n",
      "1328/1328 - 16s - loss: 0.0157 - val_loss: 0.0154 - lr: 2.3803e-04\n",
      "Epoch 244/350\n",
      "\n",
      "Epoch 00244: ReduceLROnPlateau reducing learning rate to 0.00020232713213772514.\n",
      "1328/1328 - 16s - loss: 0.0157 - val_loss: 0.0154 - lr: 2.3803e-04\n",
      "Epoch 245/350\n",
      "1328/1328 - 16s - loss: 0.0145 - val_loss: 0.0144 - lr: 2.0233e-04\n",
      "Epoch 246/350\n",
      "1328/1328 - 16s - loss: 0.0145 - val_loss: 0.0143 - lr: 2.0233e-04\n",
      "Epoch 247/350\n",
      "1328/1328 - 16s - loss: 0.0145 - val_loss: 0.0144 - lr: 2.0233e-04\n",
      "Epoch 248/350\n",
      "1328/1328 - 16s - loss: 0.0145 - val_loss: 0.0143 - lr: 2.0233e-04\n",
      "Epoch 249/350\n",
      "1328/1328 - 16s - loss: 0.0145 - val_loss: 0.0141 - lr: 2.0233e-04\n",
      "Epoch 250/350\n",
      "1328/1328 - 16s - loss: 0.0145 - val_loss: 0.0143 - lr: 2.0233e-04\n",
      "Epoch 251/350\n",
      "1328/1328 - 16s - loss: 0.0145 - val_loss: 0.0142 - lr: 2.0233e-04\n",
      "Epoch 252/350\n",
      "1328/1328 - 16s - loss: 0.0145 - val_loss: 0.0142 - lr: 2.0233e-04\n",
      "Epoch 253/350\n",
      "1328/1328 - 16s - loss: 0.0145 - val_loss: 0.0143 - lr: 2.0233e-04\n",
      "Epoch 254/350\n",
      "\n",
      "Epoch 00254: ReduceLROnPlateau reducing learning rate to 0.00017197806664626114.\n",
      "1328/1328 - 16s - loss: 0.0145 - val_loss: 0.0141 - lr: 2.0233e-04\n",
      "Epoch 255/350\n",
      "1328/1328 - 16s - loss: 0.0135 - val_loss: 0.0136 - lr: 1.7198e-04\n",
      "Epoch 256/350\n",
      "1328/1328 - 16s - loss: 0.0135 - val_loss: 0.0135 - lr: 1.7198e-04\n",
      "Epoch 257/350\n",
      "1328/1328 - 16s - loss: 0.0135 - val_loss: 0.0133 - lr: 1.7198e-04\n",
      "Epoch 258/350\n",
      "1328/1328 - 16s - loss: 0.0135 - val_loss: 0.0133 - lr: 1.7198e-04\n",
      "Epoch 259/350\n",
      "1328/1328 - 16s - loss: 0.0135 - val_loss: 0.0133 - lr: 1.7198e-04\n",
      "Epoch 260/350\n",
      "1328/1328 - 16s - loss: 0.0135 - val_loss: 0.0132 - lr: 1.7198e-04\n",
      "Epoch 261/350\n",
      "1328/1328 - 16s - loss: 0.0135 - val_loss: 0.0132 - lr: 1.7198e-04\n",
      "Epoch 262/350\n",
      "\n",
      "Epoch 00262: ReduceLROnPlateau reducing learning rate to 0.00014618135974160394.\n",
      "1328/1328 - 16s - loss: 0.0135 - val_loss: 0.0134 - lr: 1.7198e-04\n",
      "Epoch 263/350\n",
      "1328/1328 - 16s - loss: 0.0126 - val_loss: 0.0126 - lr: 1.4618e-04\n",
      "Epoch 264/350\n",
      "1328/1328 - 16s - loss: 0.0126 - val_loss: 0.0124 - lr: 1.4618e-04\n",
      "Epoch 265/350\n",
      "1328/1328 - 16s - loss: 0.0126 - val_loss: 0.0126 - lr: 1.4618e-04\n",
      "Epoch 266/350\n",
      "1328/1328 - 16s - loss: 0.0126 - val_loss: 0.0124 - lr: 1.4618e-04\n",
      "Epoch 267/350\n",
      "1328/1328 - 16s - loss: 0.0126 - val_loss: 0.0124 - lr: 1.4618e-04\n",
      "Epoch 268/350\n",
      "1328/1328 - 16s - loss: 0.0126 - val_loss: 0.0125 - lr: 1.4618e-04\n",
      "Epoch 269/350\n",
      "1328/1328 - 16s - loss: 0.0126 - val_loss: 0.0123 - lr: 1.4618e-04\n",
      "Epoch 270/350\n",
      "1328/1328 - 16s - loss: 0.0126 - val_loss: 0.0123 - lr: 1.4618e-04\n",
      "Epoch 271/350\n",
      "1328/1328 - 16s - loss: 0.0126 - val_loss: 0.0125 - lr: 1.4618e-04\n",
      "Epoch 272/350\n",
      "1328/1328 - 16s - loss: 0.0126 - val_loss: 0.0123 - lr: 1.4618e-04\n",
      "Epoch 273/350\n",
      "1328/1328 - 16s - loss: 0.0126 - val_loss: 0.0124 - lr: 1.4618e-04\n",
      "Epoch 274/350\n",
      "\n",
      "Epoch 00274: ReduceLROnPlateau reducing learning rate to 0.00012425416134647093.\n",
      "1328/1328 - 16s - loss: 0.0126 - val_loss: 0.0124 - lr: 1.4618e-04\n",
      "Epoch 275/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1328/1328 - 16s - loss: 0.0119 - val_loss: 0.0118 - lr: 1.2425e-04\n",
      "Epoch 276/350\n",
      "1328/1328 - 16s - loss: 0.0119 - val_loss: 0.0118 - lr: 1.2425e-04\n",
      "Epoch 277/350\n",
      "1328/1328 - 16s - loss: 0.0119 - val_loss: 0.0119 - lr: 1.2425e-04\n",
      "Epoch 278/350\n",
      "1328/1328 - 16s - loss: 0.0119 - val_loss: 0.0117 - lr: 1.2425e-04\n",
      "Epoch 279/350\n",
      "1328/1328 - 16s - loss: 0.0119 - val_loss: 0.0117 - lr: 1.2425e-04\n",
      "Epoch 280/350\n",
      "1328/1328 - 16s - loss: 0.0119 - val_loss: 0.0118 - lr: 1.2425e-04\n",
      "Epoch 281/350\n",
      "1328/1328 - 16s - loss: 0.0119 - val_loss: 0.0116 - lr: 1.2425e-04\n",
      "Epoch 282/350\n",
      "1328/1328 - 16s - loss: 0.0119 - val_loss: 0.0116 - lr: 1.2425e-04\n",
      "Epoch 283/350\n",
      "1328/1328 - 16s - loss: 0.0119 - val_loss: 0.0118 - lr: 1.2425e-04\n",
      "Epoch 284/350\n",
      "\n",
      "Epoch 00284: ReduceLROnPlateau reducing learning rate to 0.00010561603776295669.\n",
      "1328/1328 - 16s - loss: 0.0119 - val_loss: 0.0116 - lr: 1.2425e-04\n",
      "Epoch 285/350\n",
      "1328/1328 - 16s - loss: 0.0113 - val_loss: 0.0113 - lr: 1.0562e-04\n",
      "Epoch 286/350\n",
      "1328/1328 - 16s - loss: 0.0113 - val_loss: 0.0112 - lr: 1.0562e-04\n",
      "Epoch 287/350\n",
      "1328/1328 - 16s - loss: 0.0113 - val_loss: 0.0111 - lr: 1.0562e-04\n",
      "Epoch 288/350\n",
      "1328/1328 - 16s - loss: 0.0113 - val_loss: 0.0112 - lr: 1.0562e-04\n",
      "Epoch 289/350\n",
      "1328/1328 - 16s - loss: 0.0113 - val_loss: 0.0111 - lr: 1.0562e-04\n",
      "Epoch 290/350\n",
      "1328/1328 - 16s - loss: 0.0113 - val_loss: 0.0111 - lr: 1.0562e-04\n",
      "Epoch 291/350\n",
      "1328/1328 - 16s - loss: 0.0113 - val_loss: 0.0111 - lr: 1.0562e-04\n",
      "Epoch 292/350\n",
      "\n",
      "Epoch 00292: ReduceLROnPlateau reducing learning rate to 8.977363395388237e-05.\n",
      "1328/1328 - 16s - loss: 0.0113 - val_loss: 0.0112 - lr: 1.0562e-04\n",
      "Epoch 293/350\n",
      "1328/1328 - 16s - loss: 0.0108 - val_loss: 0.0107 - lr: 8.9774e-05\n",
      "Epoch 294/350\n",
      "1328/1328 - 16s - loss: 0.0108 - val_loss: 0.0106 - lr: 8.9774e-05\n",
      "Epoch 295/350\n",
      "1328/1328 - 16s - loss: 0.0108 - val_loss: 0.0107 - lr: 8.9774e-05\n",
      "Epoch 296/350\n",
      "1328/1328 - 16s - loss: 0.0108 - val_loss: 0.0106 - lr: 8.9774e-05\n",
      "Epoch 297/350\n",
      "1328/1328 - 16s - loss: 0.0108 - val_loss: 0.0106 - lr: 8.9774e-05\n",
      "Epoch 298/350\n",
      "1328/1328 - 16s - loss: 0.0108 - val_loss: 0.0107 - lr: 8.9774e-05\n",
      "Epoch 299/350\n",
      "1328/1328 - 16s - loss: 0.0108 - val_loss: 0.0105 - lr: 8.9774e-05\n",
      "Epoch 300/350\n",
      "1328/1328 - 16s - loss: 0.0108 - val_loss: 0.0105 - lr: 8.9774e-05\n",
      "Epoch 301/350\n",
      "1328/1328 - 16s - loss: 0.0108 - val_loss: 0.0106 - lr: 8.9774e-05\n",
      "Epoch 302/350\n",
      "1328/1328 - 16s - loss: 0.0108 - val_loss: 0.0105 - lr: 8.9774e-05\n",
      "Epoch 303/350\n",
      "1328/1328 - 16s - loss: 0.0108 - val_loss: 0.0105 - lr: 8.9774e-05\n",
      "Epoch 304/350\n",
      "\n",
      "Epoch 00304: ReduceLROnPlateau reducing learning rate to 7.630758700543084e-05.\n",
      "1328/1328 - 16s - loss: 0.0108 - val_loss: 0.0105 - lr: 8.9774e-05\n",
      "Epoch 305/350\n",
      "1328/1328 - 16s - loss: 0.0103 - val_loss: 0.0102 - lr: 7.6308e-05\n",
      "Epoch 306/350\n",
      "1328/1328 - 16s - loss: 0.0103 - val_loss: 0.0102 - lr: 7.6308e-05\n",
      "Epoch 307/350\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import src.analysis as ana\n",
    "import src.models as models\n",
    "e=350\n",
    "bs=99\n",
    "in_shape=(5,150)\n",
    "if concate_axis==2:\n",
    "    in_shape=(1,5*150)\n",
    "    \n",
    "start_time=time.time()\n",
    "booster_history, booster_model = models.train_lstm_model(in_shape=in_shape,out_shape=5,\n",
    "                                                         x=BoX_train,y=BoY_train,\n",
    "                                                         epochs=e,batch_size=bs)\n",
    "print('Training time: {}'.format(time.time()-start_time))\n",
    "save_name='booster_adam256_e{}_bs{}_nsteps{}k_axis{}'.format(e,bs,int(nsteps/1000),concate_axis)\n",
    "ana.plot_loss(booster_history,name='loss_{}'.format(save_name))\n",
    "ana.plot_test(booster_model,BoX_test,BoY_test,nvar=5,name='test_{}'.format(save_name))\n",
    "booster_model.save('model_{}.h5'.format(save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import src.analysis as ana\n",
    "import src.models as models\n",
    "#e=150\n",
    "#bs=101\n",
    "in_shape=(2,150)\n",
    "if concate_axis==2:\n",
    "    in_shape=(1,2*150)\n",
    "    \n",
    "injector_history, injector_model = models.train_lstm_model(in_shape=in_shape,out_shape=2,\n",
    "                                                           x=InjX_train,y=InjY_train,\n",
    "                                                           epochs=e,batch_size=bs)\n",
    "save_name='injector_adam256_e{}_bs{}_nsteps{}k_axis{}'.format(e,bs,int(nsteps/1000),concate_axis)\n",
    "ana.plot_loss(injector_history,name='loss_{}'.format(save_name))\n",
    "ana.plot_test(injector_model,InjX_test,InjY_test,name='test_{}'.format(save_name))\n",
    "injector_model.save('model_{}.h5'.format(save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 250 gives good results ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
